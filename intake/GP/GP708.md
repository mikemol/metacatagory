This is **Automated Ontology Engineering**. The system is building its own type system.

### **Project Nedge-Topology: The Knight's Move Manifest (v8.0)**

This manifest operationalizes the **Topological Lifecycle**.

The system now possesses the full biological rhythm of knowledge evolution:

1. **Induction (Birth):** Creating new rules from chaos.   2. **Reinforcement (Growth):** Shifting prototypes via Hebbian learning.   3. **Mitosis (Inflation):** Expanding high-tension categories into Polytopes (Stasheff Expansion).   4. **Meiosis (Compaction):** Detecting when a Polytope has become too complex ($K\_6+$) and performing **Topological Surgery** (The Knight's Move) to factorize it into simpler, stable constituents.

***

### **I. Directory Structure**

Plaintext

nedge-topology/   ├── pyproject.toml   ├── requirements.txt   ├── README.md   ├── dashboard.py             # \[UPDATED] Visualizes Surgery   ├── deploy.sh                # Deployment Vector   ├── proofs/                  # Agda Formal Verification   │   ├── ...   │   └── TopologicalCompaction.agda   └── nedge\_topology/       ├── \_\_init\_\_.py          # \[UPDATED] Exports Compactor       ├── geometry.py          # Adjoint Engine       ├── graph.py             # Nodes       ├── parser.py            # Parser       ├── search.py            # Search       ├── storage.py           # Storage       ├── visualizer.py        # Viz       ├── induction.py         # Induction       ├── mitosis.py           # Inflation       ├── stasheff\_gen.py      # Loday Generator       ├── compaction.py        # \[NEW] Topological Compactor (Knight's Move)       └── train.py             # \[UPDATED] Full Lifecycle Loop

***

### **II. Critical Module Updates**

#### **1. nedge\_topology/compaction.py (The Surgeon)**

*Detects bloated Polytopes and splits them.*

Python

import torch   import torch.nn.functional as F   import numpy as np   from sklearn.mixture import GaussianMixture   from typing import List   from .parser import TopologicalParser, GrammarRule

class TopologicalCompactor:       """       The 'Knight's Move' Engine.       Detects bloated categories and performs Topological Surgery (Factorization).       """       def \_\_init\_\_(self, parser: TopologicalParser):           self.parser = parser

    def check\_and\_compact(self, historical\_vectors: dict):           """           Scans for categories that are too complex (e.g., K6+) and splits them.           """           updates \= \[\]           \# Iterate over a copy to allow modification during iteration           for rule in list(self.parser.rules):               lhs \= rule.lhs                              \# Check complexity (Vertex Count)               if hasattr(rule, 'vertices') and rule.vertices is not None:                   complexity \= len(rule.vertices)                                      \# Heuristic: Trigger surgery at K6 (Hexagon)                   if complexity \>= 6:                       print(f"♟️ Knight's Move Triggered \[{lhs}\]: Complexity K{complexity}")                                              if lhs in historical\_vectors:                           vecs \= historical\_vectors\[lhs\]                                                      \# 1\. The Expansion (Lift & Cluster)                           \# Use GMM to detect natural separation                           try:                               gmm \= GaussianMixture(n\_components=2, covariance\_type='full', random\_state=42)                               labels \= gmm.fit\_predict(vecs.numpy())                           except Exception as e:                               print(f"   \-\> Surgery aborted (GMM failed): {e}")                               continue                                                      \# 2\. The Contraction (Factorization)                           new\_rules \= self.\_perform\_surgery(rule, vecs, labels)                                                      if new\_rules:                               print(f"   \-\> Surgery Successful: {lhs} split into {\[r.lhs for r in new\_rules\]}")                                                              \# 3\. Patch the Grammar                               if rule in self.parser.rules:                                   self.parser.rules.remove(rule)                               self.parser.rules.extend(new\_rules)                                                              \# 4\. Patch Parent References (The Infection)                               self.\_rewrite\_parents(lhs, \[r.lhs for r in new\_rules\])                                                              updates.append(lhs)           return updates

    def \_perform\_surgery(self, old\_rule: GrammarRule, vectors: torch.Tensor, labels: np.ndarray):           """Splits one Rule into Two Rules."""           mask\_a \= (labels \== 0)           vecs\_a \= vectors\[mask\_a\]           if len(vecs\_a) \< 2: return None                      rule\_a \= GrammarRule(f"{old\_rule.lhs}\_A", old\_rule.rhs, dim=old\_rule.prototype.shape\[0\])           rule\_a.prototype \= F.normalize(torch.tensor(vecs\_a).float().mean(dim=0), p=2, dim=0)                      mask\_b \= (labels \== 1)           vecs\_b \= vectors\[mask\_b\]           if len(vecs\_b) \< 2: return None                      rule\_b \= GrammarRule(f"{old\_rule.lhs}\_B", old\_rule.rhs, dim=old\_rule.prototype.shape\[0\])           rule\_b.prototype \= F.normalize(torch.tensor(vecs\_b).float().mean(dim=0), p=2, dim=0)                      \# Reset topology to simple points (K1)           rule\_a.vertices \= None           rule\_b.vertices \= None                      return \[rule\_a, rule\_b\]

    def \_rewrite\_parents(self, old\_sym, new\_syms):           """Updates parent rules to reference the new split symbols."""           new\_parent\_rules \= \[\]           rules\_to\_remove \= \[\]                      for rule in self.parser.rules:               if old\_sym in rule.rhs:                   rules\_to\_remove.append(rule)                   \# Create permutations for the split                   \# Simplified: Just branch the rule for each new symbol variant                   for new\_sym in new\_syms:                       new\_rhs \= \[new\_sym if s \== old\_sym else s for s in rule.rhs\]                       new\_parent \= GrammarRule(rule.lhs, new\_rhs, dim=rule.prototype.shape\[0\])                       \# Inherit prototype to maintain semantic stability                       new\_parent.prototype \= rule.prototype                        new\_parent\_rules.append(new\_parent)                      for r in rules\_to\_remove:               if r in self.parser.rules:                   self.parser.rules.remove(r)                      self.parser.rules.extend(new\_parent\_rules)

#### **2. nedge\_topology/train.py (The Full Lifecycle)**

*Integrated Loop: Reinforce -> Induce -> Inflate -> Compact.*

Python

import torch   import torch.nn.functional as F   import os   from typing import List   from .parser import TopologicalParser, GrammarRule   from .induction import GrammarInducer   from .mitosis import TopologicalInflator   from .compaction import TopologicalCompactor   from .storage import TopologicalStore

class ReflexiveTrainer:       def \_\_init\_\_(self, rules: List\[GrammarRule], storage\_path: str, learning\_rate: float = 0.05):           self.storage\_path = storage\_path           self.store = TopologicalStore(storage\_path)           self.learning\_rate = learning\_rate                      if os.path.exists(storage\_path):               self.parser = self.store.load(rules)           else:               self.parser = TopologicalParser(rules, dim=64)                          self.inducer = GrammarInducer(self.parser)           self.inflator = TopologicalInflator(self.parser)           self.compactor = TopologicalCompactor(self.parser)                      # Memory buffer for historical vectors (needed for Mitosis/Meiosis)           self.history = {}

    def train\_epoch(self, corpus: List\[str\]):           successes \= 0           inductions \= 0                      print(f"--- Epoch Start ({len(corpus)} items) \---")                      for sentence\_text in corpus:               tokens \= sentence\_text.lower().split()               results \= self.parser.parse(tokens)                              if len(results) \> 0:                   self.\_reinforce(results\[0\])                   self.\_record\_history(results\[0\]) \# Accumulate data for topology                   successes \+= 1               else:                   new\_rules \= self.inducer.induce(tokens)                   if new\_rules:                       self.parser.rules.extend(new\_rules)                       inductions \+= len(new\_rules)

        \# \--- END OF EPOCH TOPOLOGY MAINTENANCE \---                      \# 1\. Check for High Tension (Inflation)           print("\>\> Running Mitosis Scan...")           \# Convert history lists to tensors           tensor\_history \= {k: torch.stack(v) for k, v in self.history.items() if len(v) \> 5}           inflated \= self.inflator.check\_and\_inflate(tensor\_history, threshold=0.4)                      \# 2\. Check for Bloat (Compaction)           \# Only run if we actually have complex polytopes           if any(hasattr(r, 'vertices') and r.vertices is not None for r in self.parser.rules):               print("\>\> Running Knight's Move Scan...")               compacted \= self.compactor.check\_and\_compact(tensor\_history)                      self.store.save(self.parser)           return successes, inductions

    def \_reinforce(self, root\_item):           """Hebbian Drift."""           rule \= root\_item.rule           drift \= F.normalize(root\_item.vector\_acc, p=2, dim=0)           new\_proto \= (1.0 \- self.learning\_rate) \* rule.prototype \+ \\                       (self.learning\_rate \* drift)           rule.prototype \= F.normalize(new\_proto, p=2, dim=0)

    def \_record\_history(self, root\_item):           """Collects semantic vectors for topological analysis."""           \# Simple recursive collection           \# In full implementation, we traverse the tree.           \# Here we just record the root rule's vector.           lhs \= root\_item.rule.lhs           if lhs not in self.history: self.history\[lhs\] \= \[\]           self.history\[lhs\].append(root\_item.vector\_acc.detach())

#### **3. nedge\_topology/\_\_init\_\_.py**

Python

from .geometry import Geometry   from .graph import SPPFNode, PackedNode   from .parser import TopologicalParser, GrammarRule   from .visualizer import TopologicalVisualizer   from .search import SemanticSearchFunctor   from .storage import TopologicalStore   from .induction import GrammarInducer   from .train import ReflexiveTrainer   from .mitosis import TopologicalInflator   from .compaction import TopologicalCompactor   from .stasheff\_gen import LodayGenerator

\_\_all\_\_ = \[       "Geometry", "SPPFNode", "PackedNode",       "TopologicalParser", "GrammarRule",       "TopologicalVisualizer", "SemanticSearchFunctor",       "TopologicalStore", "GrammarInducer", "ReflexiveTrainer",       "TopologicalInflator", "TopologicalCompactor", "LodayGenerator"   ]

***

### **III. Visualizing the Surgery**

When you run dashboard.py (which simply needs to import the new compactor to be aware of it, no major changes needed as it visualizes the *state* of the parser), you will observe:

1. **Complexity:** A category NP grows to $K\_6$ (Hexagon).   2. **Trigger:** The Compactor activates.   3. **Result:** NP disappears. NP\_A and NP\_B appear, likely as $K\_1$ (Centroids) or $K\_3$ (Triangles).   4. **Tree Update:** The rule S -> NP VP disappears. S -> NP\_A VP and S -> NP\_B VP appear.
