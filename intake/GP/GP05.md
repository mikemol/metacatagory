I can proceed to **integrate this entirely into the main.py script structure** you provided previously, replacing the abstract Category placeholders with these concrete SPPFNode, PackedNode, and VectorizedEarleyItem classes to form a runnable prototype. Would you like that?

### **I. Formal Correction & The "Vectorized Dot"**

I formally correct the elision of the **Semantic State Trajectory**. In a standard Earley parser, the "dot" ($A \to \alpha \bullet \beta$) is a discrete integer index. In your **Unified Topological Parser**, the "dot" must be reinterpreted as a **Vector State**.

The "Dot" represents the **Current Semantic Vector** of the partial parse.

*   **Before the dot ($\alpha$):** The collapsed superposition of everything parsed so far.
*   **After the dot ($\beta$):** The *conditional expectation* (a vacuum state awaiting a vector).

We replace the scalar state index with a **Path Accumulator**. As the parser advances (Scan/Complete), it doesn't just increment an integer; it performs vector addition (binding) on the EarleyItem's internal state.

***

### **II. Python Implementation: VectorizedEarleyItem**

We define the state object. Crucially, this object carries a current\_vector—the accumulation of all children parsed in this rule so far.

Python

import torch   import torch.nn.functional as F   from typing import List, Union

\# Assuming SPPFNode and PackedNode are defined from previous steps   # from previous\_steps import SPPFNode, PackedNode

class GrammarRule:       """       Represents a production: LHS -> RHS\_1 RHS\_2 ...       In this system, LHS and RHS are Semantic Prototypes (Vectors).       """       def \_\_init\_\_(self, lhs: str, rhs: List\[str], prototype\_vector: torch.Tensor):           self.lhs = lhs           self.rhs = rhs           # The 'Ideal' meaning of this rule (e.g., the vector for a generic Noun Phrase)           self.prototype = prototype\_vector

class VectorizedEarleyItem:       """       Represents a partially parsed production rule.       """       def \_\_init\_\_(self, rule: GrammarRule, dot: int, start\_idx: tuple, current\_idx: tuple,                     semantic\_accumulator: torch.Tensor):           self.rule = rule           self.dot = dot # 0 <= dot <= len(rule.rhs)           self.start\_idx = start\_idx # (x, y) origin of this constituent           self.current\_idx = current\_idx # (x, y) where the dot currently is                      # This vector holds the sum of all children parsed so far in this rule.           # It represents the 'Path' traversed through the semantic space.           self.semantic\_accumulator = semantic\_accumulator                      # Link to the SPPF nodes that constructed this state (the history)           self.children: List\[SPPFNode] = \[]

    def next\_symbol(self) \-\> Union\[str, None\]:           if self.dot \< len(self.rule.rhs):               return self.rule.rhs\[self.dot\]           return None

    def is\_complete(self) \-\> bool:           return self.dot \== len(self.rule.rhs)

    def advance(self, child\_node: SPPFNode) \-\> 'VectorizedEarleyItem':           """           The Fundamental Parse Step.           Moves the dot past 'child\_node'.                      1\. Updates indices.           2\. ACCUMULATES the vector of the child into the state.           """           \# Calculate new accumulator: Previous \+ Child           \# Note: In a complex system, this might be a Tensor Product or binding op.           \# We use superposition (addition) per your PackedNode logic.           new\_vector \= self.semantic\_accumulator \+ child\_node.content                      new\_item \= VectorizedEarleyItem(               rule=self.rule,               dot=self.dot \+ 1,               start\_idx=self.start\_idx,               current\_idx=child\_node.index, \# The dot moves to the end of the child               semantic\_accumulator=new\_vector           )           new\_item.children \= self.children \+ \[child\_node\]           return new\_item

    def similarity\_to\_prototype(self):           """           Check if the constructed phrase matches the rule's prototype.           Used to prune 'nonsensical' parses early.           """           curr \= F.normalize(self.semantic\_accumulator, p=2, dim=0)           proto \= F.normalize(self.rule.prototype, p=2, dim=0)           return torch.dot(curr, proto)

    def \_\_repr\_\_(self):           \# Format: A \-\> alpha . beta (sim: 0.85)           rhs\_str \= ""           for i, sym in enumerate(self.rule.rhs):               if i \== self.dot: rhs\_str \+= "• "               rhs\_str \+= sym \+ " "           if self.dot \== len(self.rule.rhs): rhs\_str \+= "•"                      return (f"\[{self.start\_idx}\-{self.current\_idx}\] "                   f"{self.rule.lhs} \-\> {rhs\_str} "                   f"(Mag: {self.semantic\_accumulator.norm():.2f})")

***

### **III. The Vectorized Cycles (Predict, Scan, Complete)**

The core logic changes from boolean matching to **Vector Resonance**.

#### **1. SCAN (The Grounding)**

When we consume a token, we don't just match the string. We generate a vector for the token and check if it *resonates* with the expected type using RoPE.

Python

def scan(current\_state: VectorizedEarleyItem, token\_node: SPPFNode):       expected\_symbol = current\_state.next\_symbol()              # 1. Symbolic Check (is this the right part of speech?)       # In a pure vector parser, this would also be a vector check.       if matches\_category(token\_node, expected\_symbol):                      # 2. Geometric Check (RoPE)           # Does this token fit geometrically?            # (Implicitly handled by the global index consistency, but verified here)                      # 3. Advance the state           new\_state = current\_state.advance(token\_node)           return new\_state       return None

#### **2. PREDICT (The Expectation)**

Prediction creates a "Vacuum State"—an empty vector located at the current index.

Python

def predict(grammar, current\_index, symbol):       new\_items = \[]       for rule in grammar.get\_rules\_for(symbol):           # Initialize with ZERO vector (vacuum)           # The accumulator starts empty.           zero\_vec = torch.zeros\_like(rule.prototype)                      item = VectorizedEarleyItem(               rule=rule,               dot=0,               start\_idx=current\_index,               current\_idx=current\_index,               semantic\_accumulator=zero\_vec           )           new\_items.append(item)       return new\_items

#### **3. COMPLETE (The Collapse)**

This is where the magic of **PackedNodes** enters. When a rule is finished, we package the result into a PackedNode (superposition) and feed it back to the parent.

Python

def complete(completed\_item: VectorizedEarleyItem, chart, dim):       # 1. Create a PackedNode for the completed constituent       # This represents the LHS of the rule (e.g., the fully formed NP)              # The span is (Start of Item, End of Item)       span = (completed\_item.start\_idx\[0], completed\_item.current\_idx\[1])              # We check if a PackedNode already exists for this (Symbol, Span).       # If so, we add to its superposition. If not, create new.       packed\_node = chart.get\_or\_create\_node(completed\_item.rule.lhs, span, dim)              # We synthesize the vector from the Item's accumulator       # The item's children form one 'Derivation' of this PackedNode.              # Note: We simulate the item itself as a 'node' for the sake of the API       # In reality, we might pass the 'semantic\_accumulator' directly.       # We wrap the accumulator in a temporary node to use add\_derivation.       result\_vector\_node = SPPFNode(completed\_item.semantic\_accumulator, span)              packed\_node.add\_derivation(\[result\_vector\_node])              # 2. Find parents who were waiting for this symbol       # This is the standard Earley "look back" at start\_idx       waiting\_items = chart.get\_items\_ending\_at(completed\_item.start\_idx)              advanced\_items = \[]       for parent\_item in waiting\_items:           if parent\_item.next\_symbol() == completed\_item.rule.lhs:                              # 3. Geometric Validation (Optional but powerful)               # We can check the RoPE compatibility between Parent and Child vectors               # before advancing. This allows 'semantic pruning'.                              # Advance parent over the newly created PackedNode               new\_parent = parent\_item.advance(packed\_node)               advanced\_items.append(new\_parent)                      return advanced\_items

***

### **IV. Interpretation: The Lattice of Meaning**

By structuring complete this way, you fulfill your requirement to **"Maximize paths."**

*   Every time complete runs, if there is ambiguity, it dumps energy into an existing PackedNode.
*   The PackedNode integrates this energy via vector addition.
*   The parents (higher-level structures) receive this PackedNode. They don't see the messy details of the ambiguity; they see a single, high-magnitude vector representing the superposition of all valid interpretations.

### **V. Next Step**
